[{"UID":"D4","abstract":"Counterfactual prediction is a fundamental task in decision-making. This paper introduces G-Net, a sequential deep learning framework for counterfactual prediction under dynamic time varying treatment strategies in complex longitudinal settings. G-Net is based upon g-computation, a causal inference method for estimating effects of general dynamic treatment strategies. Past g-computation implementations have mostly been built using classical regression models. G-Net instead adopts a recurrent neural network framework to capture complex temporal and nonlinear dependencies in the data. To our knowledge, G-Net is the first g-computation based deep sequential modeling framework that provides estimates of treatment effects under dynamic and time-varying treatment strategies. We evaluate G-Net using simulated longitudinal data from two sources: CVSim, a mechanistic model of the cardiovascular system, and a pharmacokinetic simulation of tumor growth. G-Net outperforms both classical and state-of-the-art counterfactual prediction models in these settings.","abstract_link":"https://proceedings.mlr.press/v158/li21a.html","authors":"Rui Li*, Stephanie Hu*, Mingyu Lu, Yuria Utsumi, Prithwish Chakraborty, Daby M. Sow, Piyush Madan, Mohamed Ghalwash, Zach Shahn, Li-wei H. Lehman","chat_active":"","code_link":"","keywords":"causality","paper_link":"https://proceedings.mlr.press/v158/li21a/li21a.pdf","poster_link":"https://drive.google.com/open?id=1ETeDJwHWedm1OnnIKokeULo1OvseeyAW","presentation_link":"","title":"G-Net: a Recurrent Network Approach to G-Computation for Counterfactual Prediction Under a Dynamic Treatment Regime"},{"UID":"F5","abstract":"We introduce a method for efficient knowledge distillation of transformer-based object detectors. The proposed attention distillation makes use of the self-attention matrices generated within the layers of the state-of-art detection transformer (DETR) model. Localization information from the attention maps of a large teacher network are distilled into smaller student networks capable of running at much higher speeds. We further investigate distilling spatio-temporal information captured by 3D detection transformer networks into 2D object detectors that only process single frames. We apply the approach to the clinically important problem of detecting medical instruments in real-time from ultrasound video sequences, where inference speed is critical on computationally resource-limited hardware. We observe that, via attention distillation, student networks are able to approach the detection performance of larger teacher networks, while meeting strict computational requirements. Experiments demonstrate notable gains in accuracy and speed compared to detection transformer models trained without attention distillation.","abstract_link":"https://proceedings.mlr.press/v158/rubin21a.html","authors":"Jonathan Rubin, Ramon Erkamp, Ragha Srinivasa Naidu, Anumod Odungatta Thodiyil, Alvin Chen","chat_active":"","code_link":"","keywords":"attention, video, imaging, efficiency","paper_link":"https://proceedings.mlr.press/v158/rubin21a/rubin21a.pdf","poster_link":"https://drive.google.com/open?id=1cRTIvVg4fVJTmu7p9MikMclFlPAvWQnR","presentation_link":"","title":"Attention Distillation for Detection Transformers: Application to Real-Time Video Object Detection in Ultrasound"},{"UID":"A1","abstract":"Clinical decision support for histopathology image data mainly focuses on strongly supervised annotations, which offers intuitive interpretability, but is bound by expert performance. Here, we propose an explainable cancer relapse prediction network (eCaReNet) and show that end-to-end learning without strong annotations offers state-of-the-art performance while interpretability can be included through an attention mechanism. On the use case of prostate cancer survival prediction, using 14,479 images and only relapse times as annotations, we reach a cumulative dynamic AUC of 0.78 on a validation set, being on par with an expert pathologist (and an AUC of 0.77 on a separate test set). Our model is well-calibrated and outputs survival curves as well as a risk score and group per patient. Making use of the attention weights of a multiple instance learning layer, we show that malignant patches have a higher influence on the prediction than benign patches, thus offering an intuitive interpretation of the prediction. Our code is available at www.github.com/imsb-uke/ecarenet.","abstract_link":"https://proceedings.mlr.press/v158/dietrich21a.html","authors":"Esther Dietrich, Patrick Fuhlert, Anne Ernst, Guido Sauter, Maximilian Lennartz, H. Siegfried Stiehl, Marina Zimmermann*, Stefan Bonn*","chat_active":"","code_link":"http://www.github.com/imsb-uke/ecarenet","keywords":"explainability, attention","paper_link":"https://proceedings.mlr.press/v158/dietrich21a/dietrich21a.pdf","poster_link":"https://drive.google.com/open?id=1UOtFj0P4F0tnYb97RtoGUth5wIpE_GWK","presentation_link":"","title":"Towards Explainable End-to-End Prostate Cancer Relapse Prediction from H&E Images Combining Self-Attention Multiple Instance Learning with a Recurrent Neural Network"},{"UID":"E1","abstract":"Radiology reports are unstructured and contain the imaging findings and corresponding diagnoses transcribed by radiologists which include clinical facts and negated and/or uncertain statements. Extracting pathologic findings and diagnoses from radiology reports is important for quality control, population health, and monitoring of disease progress. Existing works, primarily rely either on rule-based systems or transformer-based pre-trained model fine-tuning, but could not take the factual and uncertain information into consideration, and therefore generate false positive outputs. In this work, we introduce three sedulous augmentation techniques which retain factual and critical information while generating augmentations for contrastive learning. We introduce RadBERT-CL, which fuses these information into BlueBert via a self-supervised contrastive loss. Our experiments on MIMIC-CXR show superior performance of RadBERT-CL on fine-tuning for multi-class, multi-label report classification. We illustrate that when few labeled data are available, RadBERT-CL outperforms conventional SOTA transformers (BERT/BlueBert) by significantly larger margins (6-11%). We also show that the representations learned by RadBERT-CL can capture critical medical information in the latent space.","abstract_link":"https://proceedings.mlr.press/v158/jaiswal21a.html","authors":"Ajay Jaiswal, Liyan Tang, Meheli Ghosh, Justin F Rousseau, Yifan Peng, Ying Ding","chat_active":"","code_link":"","keywords":"pre-training, attention, imaging, nlp","paper_link":"https://proceedings.mlr.press/v158/jaiswal21a/jaiswal21a.pdf","poster_link":"https://drive.google.com/open?id=1C7WddLDgkp0elfgmd8ZCPCg4t_8YVeEk","presentation_link":"","title":"RadBERT-CL: Factually-Aware Contrastive Learning For Radiology Report Classification"},{"UID":"H4","abstract":"Symptom checkers have emerged as an important tool for collecting symptoms and diagnosing patients, minimizing the involvement of clinical personnel. We developed a machine-learning-backed system, SmartTriage, which goes beyond conventional symptom checking through a tight bi-directional integration with the electronic medical record (EMR). Conditioned on EMR-derived patient history, our system identifies the patient's chief complaint from a free-text entry and then asks a series of discrete questions to obtain relevant symptomatology. The patient-specific data are used to predict detailed ICD-10-CM codes as well as medication, laboratory, and imaging orders. Patient responses and clinical decision support (CDS) predictions are then inserted back into the EMR. To train the machine learning components of SmartTriage, we employed novel data sets of over 25 million primary care encounters and 1 million patient free-text reason-for-visit entries. These data sets were used to construct: (1) a long short-term memory (LSTM) based patient history representation, (2) a fine-tuned transformer model for chief complaint extraction, (3) a random forest model for question sequencing, and (4) a feed-forward network for CDS predictions. In total, our system supports 337 patient chief complaints, which together make up >90% of all primary care encounters at Kaiser Permanente.","abstract_link":"https://proceedings.mlr.press/v158/valmianski21a.html","authors":"Ilya Valmianski, Nave Frost, Navdeep Sood, Yang Wang, Baodong Liu, James J. Zhu, Sunil Karumuri, Ian M. Finn, Daniel S. Zisook","chat_active":"","code_link":"","keywords":"symptom checking, ehr, nlp","paper_link":"https://proceedings.mlr.press/v158/valmianski21a/valmianski21a.pdf","poster_link":"https://drive.google.com/open?id=13CsthHS1b8CYpNu5wE3yob4JsirPXwN4","presentation_link":"","title":"SmartTriage: A system for personalized patient data capture, documentation generation, and decision support"},{"UID":"E2","abstract":"We propose CXR-RePaiR: a retrieval-based radiology report generation approach using a pre-trained contrastive language-image model. Our method generates clinically accurate reports on both in-distribution and out-of-distribution data. CXR-RePaiR outperforms or matches prior report generation methods on clinical metrics, achieving an average F1-score of 0.540 (+/-22.7%) on an external radiology dataset (CheXpert). Further, we implement a compression approach used to reduce the size of the reference corpus and speed up the runtime of our retrieval method. With compression, our model maintains similar performance while producing reports 70% faster than the best generative model. Our approach can be broadly useful in improving the diagnostic performance and generalizability of report generation models and enabling their use in clinical workflows.","abstract_link":"https://proceedings.mlr.press/v158/endo21a.html","authors":"Mark Endo*, Rayan Krishnan*, Viswesh Krishna, Andrew Y. Ng, Pranav Rajpurkar","chat_active":"","code_link":"","keywords":"pre-training, imaging, nlp","paper_link":"https://proceedings.mlr.press/v158/endo21a/endo21a.pdf","poster_link":"https://drive.google.com/open?id=1No-6XzvXA8VddIX-1IV7Y_OOD2ocdlXg","presentation_link":"","title":"Retrieval-Based Chest X-Ray Report Generation Using a Pre-trained Contrastive Language-Image Model"},{"UID":"D3","abstract":"The increase in availability of longitudinal electronic health record (EHR) data is leading to improved understanding of diseases and discovery of novel phenotypes. The majority of clustering algorithms focus only on patient trajectories, yet patients with similar trajectories may have different outcomes. Finding sub- groups of patients with different trajectories and outcomes can guide future drug development and improve recruitment to clinical trials. We develop a recurrent neural network autoencoder to cluster EHR data using reconstruction, outcome, and clustering losses which can be weighted to find different types of patient clusters. We show our model is able to discover known clusters from both data biases and outcome differences, outperforming baseline models. We demonstrate the model performance on 29,229 diabetes patients, showing it finds clusters of patients with both different trajectories and different outcomes which can be utilized to aid clinical decision making.","abstract_link":"https://proceedings.mlr.press/v158/carr21a.html","authors":"Oliver Carr*, Avelino Javer*, Patrick Rockenschaub*, Owen Parsons, Robert D\u00fcrichen","chat_active":"","code_link":"","keywords":"timeseries, ehr","paper_link":"https://proceedings.mlr.press/v158/carr21a/carr21a.pdf","poster_link":"https://drive.google.com/open?id=1tu2Wdpu5NlkNTiuJHpqCuYZegzrAd_LZ","presentation_link":"","title":"Longitudinal patient stratification of electronic health records with flexible adjustment for clinical outcomes"},{"UID":"A2","abstract":"Colorectal cancer (CRC) is among the top three most common cancers worldwide, and around 30-50% of patients who have undergone curative-intent surgery will eventually develop recurrence. Early and accurate detection of cancer recurrence is essential to improve the health outcomes of patients. In our study, we propose an explainable multi-view deep neural network capable of extracting and integrating features from heterogeneous healthcare records. Our model takes in inputs from multiple views and comprises: 1) two subnetworks adapted to extract high quality features from time-series and tabular data views, and 2) a network that combines the two outputs and predicts CRC recurrence. Our model achieves an AUROC score of 0.95, and precision, sensitivity and specificity scores of 0.84, 0.82 and 0.96 respectively, outperforming all-known published results based on the commonly-used CEA prognostic marker, as well as that of most commercially available diagnostic assays. We explain our model's decision by highlighting important features within both data views that contribute to the outcome, using SHAP with a novel workaround that alleviates assumptions on feature independence. Through our work, we hope to contribute to the adoption of AI in healthcare by creating accurate and interpretable models, leading to better post-operative management of CRC patients.","abstract_link":"https://proceedings.mlr.press/v158/ho21a.html","authors":"Ho Danliang, Iain BH Tan, Mehul Motani","chat_active":"","code_link":"","keywords":"imaging, timeseries, interpretability","paper_link":"https://proceedings.mlr.press/v158/ho21a/ho21a.pdf","poster_link":"https://drive.google.com/open?id=1UZs4yW8pWHSstPZ3n0MIbGuurKeHSuP5","presentation_link":"","title":"Prognosticating Colorectal Cancer Recurrence using an Interpretable Deep Multi-view Network"},{"UID":"F2","abstract":"We present MEDCOD, a Medically-Accurate, Emotive, Diverse, and Controllable Dialog system with a unique approach to the natural language generator module. MEDCOD has been developed and evaluated specifically for the history taking task. It integrates the advantage of a traditional modular-design approach for incorporating (medical) domain knowledge with modern deep learning to generate flexible, human-like natural language expressions. Two key aspects of the human-like character in the natural language output of MEDCOD are described in detail. First, the generated sentences are emotive and empathetic, similar to how a doctor would communicate to the patient. Second, the generated sentence structures and phrasings are varied and diverse, while maintaining medical consistency with the desired medical concept produced by the dialogue manager module of MEDCOD. Experimental results demonstrate the effectiveness of our approach in the development of a human-like medical dialogue system. Relevant code is available at https://github.com/curai/curai-research/tree/main/MEDCOD","abstract_link":"https://proceedings.mlr.press/v158/compton21a.html","authors":"Rhys Compton, Ilya Valmianski, Li Deng, Costa Huang, Namit Katariya, Xavier Amatriain, Anitha Kannan","chat_active":"","code_link":"https://github.com/curai/curai-research/tree/main/MEDCOD","keywords":"nlp","paper_link":"https://proceedings.mlr.press/v158/compton21a/compton21a.pdf","poster_link":"https://drive.google.com/open?id=1VwW8kcg2dDvkUahTFwSg4NXdDgoXEbLN","presentation_link":"","title":"MEDCOD: A Medically-Accurate, Emotive, Diverse, and Controllable Dialog System"},{"UID":"D6","abstract":"Adoption of machine learning models in healthcare requires end users' trust in the system. Models that provide additional supportive evidence for their predictions promise to facilitate adoption. We define consistent evidence to be both compatible and sufficient with respect to model predictions. We propose measures of model inconsistency and regularizers that promote more consistent evidence. We demonstrate our ideas in the context of edema severity grading from chest radiographs. We demonstrate empirically that consistent models provide competitive performance while supporting interpretation.","abstract_link":"https://proceedings.mlr.press/v158/wang21a.html","authors":"Peiqi Wang, Ruizhi Liao, Daniel Moyer, Seth Berkowitz, Steven Horng, Polina Golland","chat_active":"","code_link":"","keywords":"imaging, interpretability, robustness","paper_link":"https://proceedings.mlr.press/v158/wang21a/wang21a.pdf","poster_link":"https://drive.google.com/open?id=1P2PJTI3iduJg1dGUpIaRebZwBcsA-TcO","presentation_link":"","title":"Image Classification with Consistent Supporting Evidence"},{"UID":"F4","abstract":"Accelerated MRI shortens acquisition time by subsampling in the measurement k-space. Recovering a high-fidelity anatomical image from subsampled measurements requires close cooperation between two components: (1) a sampler that chooses the subsampling pattern and (2) a reconstructor that recovers images from incomplete measurements. In this paper, we leverage the sequential nature of MRI measurements, and propose a fully differentiable framework that jointly learns a sequential sampling policy simultaneously with a reconstruction strategy. This co-designed framework is able to adapt during acquisition in order to capture the most informative measurements for a particular target. Experimental results on the fastMRI knee dataset demonstrate that the proposed approach successfully utilizes intermediate information during the sampling process to boost reconstruction performance. In particular, our proposed method can outperform the current state-of-the-art learned k-space sampling baseline on over 96% of test samples. We also investigate the individual and collective benefits of the sequential sampling and co-design strategies.","abstract_link":"https://proceedings.mlr.press/v158/yin21a.html","authors":"Tianwei Yin*, Zihui Wu*, He Sun, Adrian V. Dalca, Yisong Yue, Katherine L. Bouman","chat_active":"","code_link":"","keywords":"imaging","paper_link":"https://proceedings.mlr.press/v158/yin21a/yin21a.pdf","poster_link":"https://drive.google.com/open?id=1VNkO0blcDMerxyU8n9T9UJlT3BazO3Df","presentation_link":"","title":"End-to-End Sequential Sampling and Reconstruction for MRI"},{"UID":"B3","abstract":"Deep learning is increasingly used for decision-making in health applications. However, commonly used deep learning models are deterministic and are therefore unable to provide any estimate of predictive uncertainty. Quantifying model uncertainty is crucial for reducing the risk of misdiagnosis by informing practitioners of low-confident predictions. To address this issue, we propose early exit ensembles, a novel framework capable of capturing predictive uncertainty via an implicit ensemble of early exits. We evaluate our approach on the task of classification using three state-of-the-art deep learning architectures applied to three medical imaging datasets. Our experiments show that early exit ensembles provide better-calibrated uncertainty compared to Monte Carlo dropout and deep ensembles using just a single forward pass of the model. Depending on the dataset and baseline, early exit ensembles can improve uncertainty metrics up to 2x, while increasing accuracy by up to 2% over its deterministic counterpart. Finally, our results suggest that by providing well-calibrated predictive uncertainty for both in- and out-of-distribution inputs, early exit ensembles have the potential to improve trustworthiness of models in high-risk medical decision-making.","abstract_link":"https://proceedings.mlr.press/v158/qendro21a.html","authors":"Alexander Campbell*, Lorena Qendro*, Pietro Lio, Cecilia Mascolo","chat_active":"","code_link":"https://github.com/ajrcampbell/early-exit-ensembles","keywords":"imaging, uncertainty","paper_link":"https://proceedings.mlr.press/v158/qendro21a/qendro21a.pdf","poster_link":"https://drive.google.com/open?id=1YEbmGlcaP_iKVQXvqAffgpqvDZWfP4y1","presentation_link":"","title":"Early Exit Ensembles for Uncertainty Quantification"},{"UID":"F3","abstract":"An intelligent machine that can answer human questions based on electronic health records (EHR-QA) has a great practical value, such as supporting clinical decisions, managing hospital administration, and medical chatbots. Previous table-based QA studies focusing on translating natural questions into table queries (NLQ2SQL), however, suffer from the unique nature of EHR data due to complex and specialized medical terminology, hence increased decoding difficulty. In this paper, we design UniQA, a unified encoder-decoder architecture for EHR-QA where natural language questions are converted to queries such as SQL or SPARQL. We also propose input masking (IM), a simple and effective method to cope with complex medical terms and various typos and better learn the SQL/SPARQL syntax. Combining the unified architecture with an effective auxiliary training objective, UniQA demonstrated a significant performance improvement against the previous state-of-the-art model for MIMICSQL* (14.2% gain), the most complex NLQ2SQL dataset in the EHR domain, and its typo-ridden versions (approximately 28.8% gain). In addition, we confirmed consistent results for the graph-based EHR-QA dataset, MIMICSPARQL*.","abstract_link":"https://proceedings.mlr.press/v158/bae21a.html","authors":"Seongsu Bae, Daeyoung Kim, Jiho Kim, Edward Choi","chat_active":"","code_link":"","keywords":"ehr, nlp","paper_link":"https://proceedings.mlr.press/v158/bae21a/bae21a.pdf","poster_link":"https://drive.google.com/open?id=1N7K88L7XD1xETzWbJL8d-CBBLJe3YWJg","presentation_link":"","title":"Question Answering for Complex Electronic Health Records Database using Unified Encoder-Decoder Architecture"},{"UID":"C3","abstract":"Embedding algorithms are increasingly used to represent clinical concepts in healthcare for improving machine learning tasks such as clinical phenotyping and disease prediction. Recent studies have adapted state-of-the-art bidirectional encoder representations from transformers (BERT) architecture to structured electronic health records (EHR) data for the generation of contextualized concept embeddings, yet do not fully incorporate temporal data across multiple clinical domains. Therefore we developed a new BERT adaptation, CEHR-BERT, to incorporate temporal information using a hybrid approach by augmenting the input to BERT using artificial time tokens, incorporating time, age, and concept embeddings, and introducing a new second learning objective for visit type. CEHR-BERT was trained on a subset of clinical data from Columbia University Irving Medical Center-New York Presbyterian Hospital, which includes 2.4M patients, spanning over three decades, and tested using 4-fold evaluation on the following prediction tasks: hospitalization, death, new heart failure (HF) diagnosis, and HF readmission. Our experiments show that CEHR-BERT outperformed existing state-of-the-art clinical BERT adaptations and baseline models across all 4 prediction tasks in both ROC-AUC and PR-AUC. CEHR-BERT also demonstrated strong few-shot learning capability, as our model trained on only 5% of data outperformed comparison models trained on the entire data set. Ablation studies to better understand the contribution of each time component showed incremental gains with every element, suggesting that CEHR-BERT\u00c3\u2022s incorporation of artificial time tokens, time/age embeddings with concept embeddings, and the addition of the second learning objective represents a promising approach for future BERT-based clinical embeddings.","abstract_link":"https://proceedings.mlr.press/v158/pang21a.html","authors":"Chao Pang, Xinzhuo Jiang, Krishna S. Kalluri, Matthew Spotnitz, RuiJun Chen, Adler Perotte, Karthik Natarajan","chat_active":"","code_link":"https://github.com/cumc-dbmi/cehr-bert","keywords":"timeseries, ehr","paper_link":"https://proceedings.mlr.press/v158/pang21a/pang21a.pdf","poster_link":"https://drive.google.com/open?id=1sdtcbGbaFPH1yvzL8dO-SNbGdK4mxR5J","presentation_link":"","title":"CEHR-BERT: Incorporating temporal information from structured EHR data to improve prediction tasks"},{"UID":"A3","abstract":"Transfer learning has become a standard practice to mitigate the lack of labeled data in medical classification tasks. Whereas finetuning a downstream task using supervised ImageNet pretrained features is straightforward and extensively investigated in many works, there is little study on the usefulness of self-supervised pretraining. This paper assesses the transferability of the most recent self-supervised ImageNet models, including SimCLR, SwAV, and DINO, on selected medical imaging classification tasks. The chosen tasks cover tumor detection in sentinel axillary lymph node images, diabetic retinopathy classification in fundus images, and multiple pathological condition classification in chest X-ray images. We demonstrate that self-supervised pretrained models yield richer embeddings than their supervised counterparts, benefiting downstream tasks for linear evaluation and finetuning. For example, at a critically small subset of the data with linear evaluation, we see an improvement up to 14.79% in Kappa score in the diabetic retinopathy classification task, 5.4% in AUC in the tumor classification task, 7.03% AUC in the pneumonia detection, and 9.4% in AUC in the detection of pathological conditions in chest X-ray. In addition, we introduce Dynamic Visual Meta-Embedding (DVME) as an end-to-end transfer learning approach that fuses pretrained embeddings from multiple models. We show that the collective representation obtained by DVME leads to a significant improvement in the performance of selected tasks compared to using a single pretrained model approach and can be generalized to any combination of pretrained models.","abstract_link":"https://proceedings.mlr.press/v158/truong21a.html","authors":"Tuan Truong, Sadegh Mohammadi, Matthias Lenga","chat_active":"","code_link":"","keywords":"imaging, robustness","paper_link":"https://proceedings.mlr.press/v158/truong21a/truong21a.pdf","poster_link":"https://drive.google.com/open?id=170P-qQI-nwhNGMphazTG0HX_TVU0hE1B","presentation_link":"","title":"How Transferable are Self-supervised Features in Medical Image Classification Tasks?"},{"UID":"G4","abstract":"This paper presents a domain-guided approach for learning representations of scalp-electroencephalograms (EEGs) without relying on expert annotations. Expert labeling of EEGs has proven to be an unscalable process with low inter-reviewer agreement because of the complex and lengthy nature of EEG recordings. Hence, there is a need for machine learning (ML) approaches that can leverage expert domain knowledge without incurring the cost of labor-intensive annotations. Self-supervised learning (SSL) has shown promise in such settings, although existing SSL efforts on EEG data do not fully exploit EEG domain knowledge. Furthermore, it is unclear to what extent SSL models generalize to unseen tasks and datasets. Here we explore whether SSL tasks derived in a domain-guided fashion can learn generalizable EEG representations. Our contributions are three-fold: 1) we propose novel SSL tasks for EEG based on the spatial similarity of brain activity, underlying behavioral states, and age-related differences; 2) we present evidence that an encoder pretrained using the proposed SSL tasks shows strong predictive performance on multiple downstream classifications; and 3) using two large EEG datasets, we show that our encoder generalizes well to multiple EEG datasets during downstream evaluations.","abstract_link":"https://proceedings.mlr.press/v158/wagh21a.html","authors":"Neeraj Wagh, Jionghao Wei, Samarth Rawal, Leland Barnard, Benjamin Brinkmann, Brent Berry, Gregory Worrell, David Jones, Yogatheesan Varatharajah","chat_active":"","code_link":"https://github.com/neerajwagh/eeg-self-supervision","keywords":"self-supervision, neuro","paper_link":"https://proceedings.mlr.press/v158/wagh21a/wagh21a.pdf","poster_link":"https://drive.google.com/open?id=1JTqkgtycejz5y-id_NTJpND_oGaW0zmx","presentation_link":"","title":"Domain-guided Self-supervision of EEG Data Improves Downstream Classification Performance and Generalizability"},{"UID":"C4","abstract":"Estimating individualized treatment effects (ITEs) from observational data is crucial for decision-making. In order to obtain unbiased ITE estimates, a common assumption is that all confounders are observed. However, in practice, it is unlikely that we observe these confounders directly. Instead, we often observe noisy measurements of true confounders, which can serve as valid proxies. In this paper, we address the problem of estimating ITE in the longitudinal setting where we observe noisy proxies instead of true confounders. To this end, we develop the Deconfounding Temporal Autoencoder, a novel method that leverages observed noisy proxies to learn a hidden embedding that reflects the true hidden confounders. In particular, the DTA combines a long short-term memory autoencoder with a causal regularization penalty that renders the potential outcomes and treatment assignment conditionally independent given the learned hidden embedding. Once the hidden embedding is learned via DTA, state-of-the-art outcome models can be used to control for it and obtain unbiased estimates of ITE. Using synthetic and real-world medical data, we demonstrate the effectiveness of our DTA by improving over state-of-the-art benchmarks by a substantial margin.","abstract_link":"https://proceedings.mlr.press/v158/kuzmanovic21a.html","authors":"Milan Kuzmanovic, Tobias Hatt, Stefan Feuerriegel","chat_active":"","code_link":"https://github.com/mkuzma96/DTA","keywords":"timeseries, causality","paper_link":"https://proceedings.mlr.press/v158/kuzmanovic21a/kuzmanovic21a.pdf","poster_link":"https://drive.google.com/open?id=1wHq7dPiSGSkpOXEWO_iXDYk9k_I1TDgk","presentation_link":"","title":"Deconfounding Temporal Autoencoder: Estimating Treatment Effects over Time Using Noisy Proxies"},{"UID":"E3","abstract":"We propose 3KG, a physiologically-inspired contrastive learning approach that generates views using 3D augmentations of the 12-lead electrocardiogram. We evaluate representation quality by fine-tuning a linear layer for the downstream task of 23-class diagnosis on the PhysioNet 2020 challenge training data and find that 3KG achieves a 9.1% increase in mean AUC over the best self-supervised baseline when trained on 1% of labeled data. Our empirical analysis shows that combining spatial and temporal augmentations produces the strongest representations. In addition, we investigate the effect of this physiologically-inspired pretraining on downstream performance on different disease subgroups and find that 3KG makes the greatest gains for conduction and rhythm abnormalities. Our method allows for flexibility in incorporating other self-supervised strategies and highlights the potential for similar modality-specific augmentations for other biomedical signals.","abstract_link":"https://proceedings.mlr.press/v158/gopal21a.html","authors":"Bryan Gopal*, Ryan Han*, Gautham Raghupathi*, Andrew Ng, Geoff Tison**, Pranav Rajpurkar**","chat_active":"","code_link":"","keywords":"ekg, pre-training","paper_link":"https://proceedings.mlr.press/v158/gopal21a/gopal21a.pdf","poster_link":"https://drive.google.com/open?id=1fOdYf3l5jkZfjysPDWKOJGzwwnxdhAOE","presentation_link":"","title":"3KG: Contrastive Learning of 12-Lead Electrocardiograms using Physiologically-Inspired Augmentations"}]
